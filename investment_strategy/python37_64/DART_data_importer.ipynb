{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DART Open API를 이용해 기업 공시 정보를 가져온다. \n",
    "\n",
    "DART Open API 사용 연습\n",
    "\n",
    "해당 코드는 Python 3.5/3.7, 32bit/64bit 상관 없다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from urllib.request import Request\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import re\n",
    "import json\n",
    "import xml.etree.ElementTree as elemTree\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DART Open API와 연결\n",
    "\n",
    "DART API에는 4가지 정보가 있고, 각 정보는 더 세부적으로 나뉜다. \n",
    "1. 공시정보\n",
    "2. 사업보고서 주요정보\n",
    "3. 상장기업 재무정보\n",
    "4. 지분공시 종합정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./DART_password.txt', 'r') as f:\n",
    "    API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "crtfc_key = '?crtfc_key=' + API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 공시정보: \n",
    "    - 공시검색: 공시 유형별, 회사별, 날짜별 등 여러가지 조건으로 공시보고서 검색기능을 제공합니다.\n",
    "    - 기업개황: DART에 등록되어있는 기업의 개황정보를 제공합니다.\n",
    "    - 공시서류원본파일: 공시보고서 원본파일을 제공합니다.\n",
    "    - 고유번호: DART에 등록되어있는 공시대상회사의 고유번호,회사명,대표자명,종목코드, 최근변경일자를 파일로 제공합니다.\n",
    "\n",
    "기타 세부사항은 API doc에서 확인: https://opendart.fss.or.kr/guide/detail.do?apiGrpCd=DS001&apiId=2019001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 공시정보 base URLs\n",
    "\n",
    "DART_list_json = 'https://opendart.fss.or.kr/api/list.json' # 공시검색\n",
    "DART_company_json = 'https://opendart.fss.or.kr/api/company.json' # 기업개황\n",
    "DART_document_xml = 'https://opendart.fss.or.kr/api/document.xml' # 공시서류원본파일\n",
    "DART_corpCode_xml = 'https://opendart.fss.or.kr/api/corpCode.xml' # 고유번호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DART_annc_info(info_type, **kwargs):\n",
    "    \"\"\"Create a request url that includes given parameters. \n",
    "    \n",
    "    Args:\n",
    "        info_type (str): Type of info to request\n",
    "        \n",
    "    Kwargs:\n",
    "        Too many. Refer to the API doc link above. \n",
    "        \n",
    "    Returns:\n",
    "        str.\n",
    "        A complete url to hand over to Open DART API. \n",
    "    \n",
    "    \"\"\"\n",
    "    parameters = ''\n",
    "    for k, v in kwargs.items():\n",
    "        parameters += '&' + str(k) + '=' + str(v)\n",
    "    \n",
    "    if info_type == 'list':\n",
    "        return DART_list_json + crtfc_key + parameters\n",
    "    elif info_type == 'company':\n",
    "        return DART_company_json + crtfc_key + parameters\n",
    "    elif info_type == 'document':\n",
    "        return DART_document_xml + crtfc_key + parameters\n",
    "    elif info_type == 'corpCode':\n",
    "        return DART_corpCode_xml + crtfc_key + parameters\n",
    "    else:\n",
    "        print('Wrong info_type. Choose from:')\n",
    "        print('''\n",
    "        1. \"list\": 공시검색\n",
    "        2. \"company\": 기업개활\n",
    "        3. \"document\": 공시서류원본파일\n",
    "        4. \"corpCode\": 고유번호\n",
    "        ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DART_get_response(request_url):\n",
    "    \"\"\"Get response from Open DART API. \n",
    "    \n",
    "    Args: \n",
    "        request_url (str): The url to request. \n",
    "        \n",
    "    Returns:\n",
    "        tuple.\n",
    "        (\n",
    "            (str) type of the object,\n",
    "            xml or json object\n",
    "        )\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    req = urlopen(request_url)\n",
    "    response = req.read().decode('utf8')\n",
    "    \n",
    "    try:\n",
    "        result = ('json', json.loads(response))\n",
    "    except JSONDecodeError:\n",
    "        result = ('xml', elemTree.fromstring(response))\n",
    "    except:\n",
    "        print(\"An error occurred: \", sys.exc_info()[0])\n",
    "        return 0\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://opendart.fss.or.kr/api/list.json?crtfc_key=407c1fe7fc7a1a183002c6d5f981408662cd879e&corp_code=00919966&bgn_de=20130801&end_de=20150815'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req_url = DART_annc_info('list', corp_code='00919966', bgn_de='20130801', end_de='20150815')\n",
    "req_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('json',\n",
       " {'status': '000',\n",
       "  'message': '정상',\n",
       "  'page_no': 1,\n",
       "  'page_count': 10,\n",
       "  'total_count': 9,\n",
       "  'total_page': 1,\n",
       "  'list': [{'corp_code': '00919966',\n",
       "    'corp_name': '신라젠',\n",
       "    'stock_code': '215600',\n",
       "    'corp_cls': 'K',\n",
       "    'report_nm': '분기보고서 (2015.03)',\n",
       "    'rcept_no': '20150601000841',\n",
       "    'flr_nm': '신라젠',\n",
       "    'rcept_dt': '20150601',\n",
       "    'rm': '정'},\n",
       "   {'corp_code': '00919966',\n",
       "    'corp_name': '신라젠',\n",
       "    'stock_code': '215600',\n",
       "    'corp_cls': 'K',\n",
       "    'report_nm': '주요사항보고서(중요한자산양수도결정)',\n",
       "    'rcept_no': '20150430001501',\n",
       "    'flr_nm': '신라젠',\n",
       "    'rcept_dt': '20150430',\n",
       "    'rm': ''},\n",
       "   {'corp_code': '00919966',\n",
       "    'corp_name': '신라젠',\n",
       "    'stock_code': '215600',\n",
       "    'corp_cls': 'K',\n",
       "    'report_nm': '[기재정정]사업보고서 (2014.12)',\n",
       "    'rcept_no': '20150423000246',\n",
       "    'flr_nm': '신라젠',\n",
       "    'rcept_dt': '20150423',\n",
       "    'rm': '연'},\n",
       "   {'corp_code': '00919966',\n",
       "    'corp_name': '신라젠',\n",
       "    'stock_code': '215600',\n",
       "    'corp_cls': 'K',\n",
       "    'report_nm': '[기재정정]사업보고서 (2014.12)',\n",
       "    'rcept_no': '20150420000169',\n",
       "    'flr_nm': '신라젠',\n",
       "    'rcept_dt': '20150420',\n",
       "    'rm': '정연'},\n",
       "   {'corp_code': '00919966',\n",
       "    'corp_name': '신라젠',\n",
       "    'stock_code': '215600',\n",
       "    'corp_cls': 'K',\n",
       "    'report_nm': '사업보고서 (2014.12)',\n",
       "    'rcept_no': '20150415000002',\n",
       "    'flr_nm': '신라젠',\n",
       "    'rcept_dt': '20150415',\n",
       "    'rm': '정연'},\n",
       "   {'corp_code': '00919966',\n",
       "    'corp_name': '신라젠',\n",
       "    'stock_code': '215600',\n",
       "    'corp_cls': 'K',\n",
       "    'report_nm': '연결감사보고서 (2014.12)',\n",
       "    'rcept_no': '20150414001753',\n",
       "    'flr_nm': '삼일회계법인',\n",
       "    'rcept_dt': '20150414',\n",
       "    'rm': ''},\n",
       "   {'corp_code': '00919966',\n",
       "    'corp_name': '신라젠',\n",
       "    'stock_code': '215600',\n",
       "    'corp_cls': 'K',\n",
       "    'report_nm': '감사보고서 (2014.12)',\n",
       "    'rcept_no': '20150408001181',\n",
       "    'flr_nm': '삼일회계법인',\n",
       "    'rcept_dt': '20150408',\n",
       "    'rm': ''},\n",
       "   {'corp_code': '00919966',\n",
       "    'corp_name': '신라젠',\n",
       "    'stock_code': '215600',\n",
       "    'corp_cls': 'K',\n",
       "    'report_nm': '[기재정정]감사보고서 (2013.12)',\n",
       "    'rcept_no': '20140327000924',\n",
       "    'flr_nm': '남일회계법인',\n",
       "    'rcept_dt': '20140327',\n",
       "    'rm': ''},\n",
       "   {'corp_code': '00919966',\n",
       "    'corp_name': '신라젠',\n",
       "    'stock_code': '215600',\n",
       "    'corp_cls': 'K',\n",
       "    'report_nm': '감사보고서 (2013.12)',\n",
       "    'rcept_no': '20140227000130',\n",
       "    'flr_nm': '남일회계법인',\n",
       "    'rcept_dt': '20140227',\n",
       "    'rm': '정'}]})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DART_get_response(req_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 공시시간 크롤링\n",
    "\n",
    "문의결과, DART API는 현재 공시시간 정보를 제공하지 않는다. (아직 시범운영기간임을 감안하긴 해야한다.)\n",
    "\n",
    "따라서, 최근 공시 페이지는 직접 크롤링하기로 하였다. 다소 번거롭지만 API에서 지원이 되기 전까진 DART API에서 기본적인 공시 정보를 가져오고, 분단위의 공시시간이 필요한 경우 해당 날짜의 최근 공시를 크롤링한 결과와 대조해 결과를 매칭시키도록 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recent_annc_list_bs(date):\n",
    "    \"\"\"Scrape recent announcements of a specific date. \n",
    "    \n",
    "    Args: \n",
    "        date (str): The date to scrape. Should be in YYYY.MM.DD format. \n",
    "        \n",
    "    Returns:\n",
    "        list.\n",
    "        The list of beautifulsoup objects that each contains an announcement. \n",
    "    \n",
    "    \"\"\"\n",
    "    date_regex = re.compile(r'^\\d{4}\\.\\d{2}\\.\\d{2}$')\n",
    "    if not date_regex.match(date):\n",
    "        print(\"Error: Date format should be - yyyy.mm.dd\")\n",
    "        return 0\n",
    "    \n",
    "    recent_annc_list_bs = []\n",
    "    \n",
    "    for page in range(1, 11):\n",
    "        recent_annc_url = f'http://dart.fss.or.kr/dsac001/mainK.do?selectDate={date}&currentPage={page}&sort=&series=&mdayCnt=0#'\n",
    "        recent_annc_req = urlopen(recent_annc_url)\n",
    "        recent_annc_bs = bs(recent_annc_req, 'html.parser')\n",
    "        recent_annc_list_bs += recent_annc_bs.select('div.table_list > table > tr')\n",
    "    \n",
    "    recent_annc_list_bs = [x for x in recent_annc_list_bs if '검색된 자료가 없습니다.' not in x.text]\n",
    "    \n",
    "    return recent_annc_list_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annc_bs2data(annc_bs):\n",
    "    \"\"\"Convert an announcement's beautifulsoup object to a dictionary data. \n",
    "    \n",
    "    Args: \n",
    "        annc_bs (bs object): An announcement's beautifulsoup object. \n",
    "        \n",
    "    Returns:\n",
    "        dict.\n",
    "        {\n",
    "            'annc_time': ,\n",
    "            'corp_code': ,\n",
    "            'annc_title': , \n",
    "            'annc_id': ,\n",
    "        }\n",
    "    \n",
    "    \"\"\"\n",
    "    annc_time_regex = re.compile(r'\\d\\d:\\d\\d')\n",
    "    annc_time = annc_bs.find('td', attrs={'class':'cen_txt'}).text\n",
    "    annc_time = annc_time_regex.search(annc_time).group()\n",
    "    \n",
    "    corp_code_regex = re.compile(r'\\d{8}')\n",
    "    corp_code = annc_bs.find('span', {'class':'nobr1'}).a.attrs['onclick']\n",
    "    corp_code = corp_code_regex.search(corp_code).group()\n",
    "    \n",
    "    annc_content_regex = re.compile(r'openReportViewer')\n",
    "    annc_content = annc_bs.find('a', attrs={'onclick':annc_content_regex})  \n",
    "    \n",
    "    annc_title_regex = re.compile('\\\\\\\\.')\n",
    "    annc_content_text = annc_content.text \n",
    "    str_text = \"%r\"%annc_content_text # raw string으로 변환시켜줘야 \\t가 tab으로 인식되지 않는다. \n",
    "    raw_text = str_text[1:-1]\n",
    "    \n",
    "    annc_title = re.sub(annc_title_regex, '', raw_text).strip() \n",
    "    \n",
    "    annc_id_regex = re.compile(r'\\d+')\n",
    "    annc_id = annc_id_regex.search(annc_content.attrs['id']).group()\n",
    "    \n",
    "    data = {}\n",
    "    data['annc_time'] = annc_time\n",
    "    data['corp_code'] = corp_code\n",
    "    data['annc_title'] = annc_title\n",
    "    data['annc_id'] = annc_id\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recent_anncs2df(start_date, end_date, save=False, logging=False, delay=(50, 300, 2)):\n",
    "    \"\"\"The main function of DART recent announcements scraping. Scrape data in given date range and convert them to pandas df. \n",
    "    \n",
    "    Args: \n",
    "        start_date (str or int): Start date.\n",
    "        end_date (str or int): End date.\n",
    "        save (bool): Save to .pkl if True.\n",
    "        logging (bool): Print logs if True.\n",
    "        delay (tuple): (\n",
    "                        how many iterations before long pause, \n",
    "                        long pause seconds, \n",
    "                        each iteration pause seconds\n",
    "                        )\n",
    "        \n",
    "    Returns:\n",
    "        pandas dataframe\n",
    "        Pandas dataframe of announcement data in given date range. \n",
    "        columns = ['datetime', 'corp_code', 'annc_title', 'annc_id']\n",
    "    \n",
    "    \"\"\"\n",
    "    start_date = dateutil.parser.parse(str(start_date))\n",
    "    end_date = dateutil.parser.parse(str(end_date))\n",
    "    date_range = pd.date_range(start=start_date, end=end_date).tolist()\n",
    "    \n",
    "    all_anncs_df = pd.DataFrame(columns=['date', 'annc_time', 'corp_code', 'annc_title', 'annc_id'])\n",
    "    error_dates =[]\n",
    "    \n",
    "    for i, date in enumerate(date_range):\n",
    "        \n",
    "        try:\n",
    "            time.sleep(delay[2])\n",
    "            if (i != 0) and (i % delay[0] == 0) and logging:\n",
    "                print(f'sleeping for {delay[1]} seconds...')\n",
    "                time.sleep(delay[1])\n",
    "\n",
    "            anncs_df = pd.DataFrame(columns=['date', 'annc_time', 'corp_code', 'annc_title', 'annc_id'])\n",
    "            anncs_of_the_day = get_recent_annc_list_bs(date.strftime('%Y.%m.%d'))\n",
    "            anncs_of_the_day = [annc_bs2data(annc) for annc in anncs_of_the_day]\n",
    "            anncs_df = anncs_df.append(pd.DataFrame(anncs_of_the_day)) \n",
    "            anncs_df.date = date\n",
    "\n",
    "            all_anncs_df = all_anncs_df.append(anncs_df)\n",
    "\n",
    "            if logging:\n",
    "                print(f'Added data of {date}')\n",
    "        except:\n",
    "            print(f'Error occured at {date}. Sleeping for {delay[1]} seconds...')\n",
    "            error_dates.append(date)\n",
    "            time.sleep(delay[1])\n",
    "            continue\n",
    "    \n",
    "    all_anncs_df.loc[:, 'datetime'] = pd.to_datetime(all_anncs_df.date.astype(str) + ' ' + all_anncs_df.annc_time)\n",
    "    all_anncs_df.drop(['date', 'annc_time'], axis=1, inplace=True)\n",
    "    \n",
    "    if save:\n",
    "        all_anncs_df.to_pickle(f\"./all_anncs_df_{start_date.strftime('%Y.%m.%d')}-{end_date.strftime('%Y.%m.%d')}.pkl\")\n",
    "        with open(f'./error_dates_{start_date.strftime('%Y.%m.%d')}-{end_date.strftime('%Y.%m.%d')}.txt', 'w') as f:\n",
    "            f.writelines(\"{}\\n\".format(err_date) for err_date in error_dates)\n",
    "    \n",
    "    if error_dates != []:\n",
    "        print(f'There were error with these dates:')\n",
    "        for err_date in error_dates:\n",
    "            print(err_date)\n",
    "        \n",
    "    return all_anncs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added data of 2014-01-01 00:00:00\n",
      "Added data of 2014-01-02 00:00:00\n",
      "Added data of 2014-01-03 00:00:00\n",
      "Added data of 2014-01-04 00:00:00\n",
      "Added data of 2014-01-05 00:00:00\n",
      "Added data of 2014-01-06 00:00:00\n",
      "Added data of 2014-01-07 00:00:00\n",
      "Added data of 2014-01-08 00:00:00\n",
      "Added data of 2014-01-09 00:00:00\n",
      "Added data of 2014-01-10 00:00:00\n",
      "Added data of 2014-01-11 00:00:00\n",
      "Added data of 2014-01-12 00:00:00\n",
      "Added data of 2014-01-13 00:00:00\n",
      "Added data of 2014-01-14 00:00:00\n",
      "Added data of 2014-01-15 00:00:00\n",
      "Added data of 2014-01-16 00:00:00\n",
      "Added data of 2014-01-17 00:00:00\n",
      "Added data of 2014-01-18 00:00:00\n",
      "Added data of 2014-01-19 00:00:00\n",
      "Added data of 2014-01-20 00:00:00\n",
      "Added data of 2014-01-21 00:00:00\n",
      "Added data of 2014-01-22 00:00:00\n",
      "Added data of 2014-01-23 00:00:00\n",
      "Added data of 2014-01-24 00:00:00\n",
      "Added data of 2014-01-25 00:00:00\n",
      "Added data of 2014-01-26 00:00:00\n",
      "Added data of 2014-01-27 00:00:00\n",
      "Added data of 2014-01-28 00:00:00\n",
      "Added data of 2014-01-29 00:00:00\n",
      "Added data of 2014-01-30 00:00:00\n",
      "Added data of 2014-01-31 00:00:00\n",
      "Added data of 2014-02-01 00:00:00\n",
      "Added data of 2014-02-02 00:00:00\n",
      "Added data of 2014-02-03 00:00:00\n",
      "Added data of 2014-02-04 00:00:00\n",
      "Added data of 2014-02-05 00:00:00\n",
      "Added data of 2014-02-06 00:00:00\n",
      "Added data of 2014-02-07 00:00:00\n",
      "Added data of 2014-02-08 00:00:00\n",
      "Added data of 2014-02-09 00:00:00\n",
      "Added data of 2014-02-10 00:00:00\n",
      "Added data of 2014-02-11 00:00:00\n",
      "Added data of 2014-02-12 00:00:00\n",
      "Added data of 2014-02-13 00:00:00\n",
      "Added data of 2014-02-14 00:00:00\n",
      "Added data of 2014-02-15 00:00:00\n",
      "Added data of 2014-02-16 00:00:00\n",
      "Added data of 2014-02-17 00:00:00\n",
      "Added data of 2014-02-18 00:00:00\n",
      "Added data of 2014-02-19 00:00:00\n",
      "sleeping for 300 seconds...\n",
      "Added data of 2014-02-20 00:00:00\n",
      "Added data of 2014-02-21 00:00:00\n",
      "Added data of 2014-02-22 00:00:00\n",
      "Added data of 2014-02-23 00:00:00\n",
      "Added data of 2014-02-24 00:00:00\n",
      "Added data of 2014-02-25 00:00:00\n",
      "Added data of 2014-02-26 00:00:00\n",
      "Added data of 2014-02-27 00:00:00\n",
      "Added data of 2014-02-28 00:00:00\n",
      "Added data of 2014-03-01 00:00:00\n",
      "Added data of 2014-03-02 00:00:00\n",
      "Added data of 2014-03-03 00:00:00\n",
      "Added data of 2014-03-04 00:00:00\n",
      "Added data of 2014-03-05 00:00:00\n",
      "Added data of 2014-03-06 00:00:00\n",
      "Added data of 2014-03-07 00:00:00\n",
      "Added data of 2014-03-08 00:00:00\n",
      "Added data of 2014-03-09 00:00:00\n",
      "Added data of 2014-03-10 00:00:00\n",
      "Added data of 2014-03-11 00:00:00\n",
      "Added data of 2014-03-12 00:00:00\n",
      "Added data of 2014-03-13 00:00:00\n",
      "Added data of 2014-03-14 00:00:00\n",
      "Added data of 2014-03-15 00:00:00\n",
      "Added data of 2014-03-16 00:00:00\n",
      "Added data of 2014-03-17 00:00:00\n",
      "Added data of 2014-03-18 00:00:00\n",
      "Added data of 2014-03-19 00:00:00\n",
      "Added data of 2014-03-20 00:00:00\n",
      "Added data of 2014-03-21 00:00:00\n",
      "Added data of 2014-03-22 00:00:00\n",
      "Added data of 2014-03-23 00:00:00\n",
      "Error occured at 2014-03-24 00:00:00\n",
      "Added data of 2014-03-25 00:00:00\n",
      "Added data of 2014-03-26 00:00:00\n",
      "Added data of 2014-03-27 00:00:00\n",
      "Added data of 2014-03-28 00:00:00\n",
      "Added data of 2014-03-29 00:00:00\n",
      "Added data of 2014-03-30 00:00:00\n",
      "Added data of 2014-03-31 00:00:00\n",
      "Added data of 2014-04-01 00:00:00\n",
      "Added data of 2014-04-02 00:00:00\n",
      "Added data of 2014-04-03 00:00:00\n",
      "Added data of 2014-04-04 00:00:00\n",
      "Added data of 2014-04-05 00:00:00\n",
      "Added data of 2014-04-06 00:00:00\n",
      "Added data of 2014-04-07 00:00:00\n",
      "Added data of 2014-04-08 00:00:00\n",
      "Added data of 2014-04-09 00:00:00\n",
      "Added data of 2014-04-10 00:00:00\n",
      "sleeping for 300 seconds...\n",
      "Added data of 2014-04-11 00:00:00\n",
      "Added data of 2014-04-12 00:00:00\n",
      "Added data of 2014-04-13 00:00:00\n",
      "Added data of 2014-04-14 00:00:00\n",
      "Added data of 2014-04-15 00:00:00\n",
      "Added data of 2014-04-16 00:00:00\n",
      "Added data of 2014-04-17 00:00:00\n",
      "Added data of 2014-04-18 00:00:00\n",
      "Added data of 2014-04-19 00:00:00\n",
      "Added data of 2014-04-20 00:00:00\n",
      "Added data of 2014-04-21 00:00:00\n",
      "Added data of 2014-04-22 00:00:00\n",
      "Added data of 2014-04-23 00:00:00\n",
      "Added data of 2014-04-24 00:00:00\n",
      "Added data of 2014-04-25 00:00:00\n",
      "Added data of 2014-04-26 00:00:00\n",
      "Added data of 2014-04-27 00:00:00\n",
      "Added data of 2014-04-28 00:00:00\n",
      "Added data of 2014-04-29 00:00:00\n",
      "Added data of 2014-04-30 00:00:00\n",
      "Added data of 2014-05-01 00:00:00\n",
      "Added data of 2014-05-02 00:00:00\n",
      "Added data of 2014-05-03 00:00:00\n",
      "Added data of 2014-05-04 00:00:00\n",
      "Added data of 2014-05-05 00:00:00\n",
      "Added data of 2014-05-06 00:00:00\n",
      "Added data of 2014-05-07 00:00:00\n",
      "Added data of 2014-05-08 00:00:00\n",
      "Added data of 2014-05-09 00:00:00\n",
      "Added data of 2014-05-10 00:00:00\n",
      "Added data of 2014-05-11 00:00:00\n",
      "Added data of 2014-05-12 00:00:00\n",
      "Added data of 2014-05-13 00:00:00\n",
      "Added data of 2014-05-14 00:00:00\n",
      "Added data of 2014-05-15 00:00:00\n",
      "Added data of 2014-05-16 00:00:00\n",
      "Added data of 2014-05-17 00:00:00\n",
      "Added data of 2014-05-18 00:00:00\n",
      "Added data of 2014-05-19 00:00:00\n",
      "Added data of 2014-05-20 00:00:00\n",
      "Added data of 2014-05-21 00:00:00\n",
      "Added data of 2014-05-22 00:00:00\n",
      "Added data of 2014-05-23 00:00:00\n",
      "Added data of 2014-05-24 00:00:00\n",
      "Added data of 2014-05-25 00:00:00\n",
      "Added data of 2014-05-26 00:00:00\n",
      "Added data of 2014-05-27 00:00:00\n",
      "Error occured at 2014-05-28 00:00:00\n",
      "Added data of 2014-05-29 00:00:00\n",
      "Added data of 2014-05-30 00:00:00\n",
      "sleeping for 300 seconds...\n",
      "Added data of 2014-05-31 00:00:00\n",
      "Added data of 2014-06-01 00:00:00\n",
      "Added data of 2014-06-02 00:00:00\n",
      "Added data of 2014-06-03 00:00:00\n",
      "Added data of 2014-06-04 00:00:00\n",
      "Added data of 2014-06-05 00:00:00\n",
      "Added data of 2014-06-06 00:00:00\n",
      "Added data of 2014-06-07 00:00:00\n",
      "Added data of 2014-06-08 00:00:00\n",
      "Added data of 2014-06-09 00:00:00\n",
      "Added data of 2014-06-10 00:00:00\n",
      "Added data of 2014-06-11 00:00:00\n",
      "Added data of 2014-06-12 00:00:00\n",
      "Added data of 2014-06-13 00:00:00\n",
      "Added data of 2014-06-14 00:00:00\n",
      "Added data of 2014-06-15 00:00:00\n",
      "Added data of 2014-06-16 00:00:00\n",
      "Added data of 2014-06-17 00:00:00\n",
      "Added data of 2014-06-18 00:00:00\n",
      "Added data of 2014-06-19 00:00:00\n",
      "Added data of 2014-06-20 00:00:00\n",
      "Added data of 2014-06-21 00:00:00\n",
      "Added data of 2014-06-22 00:00:00\n",
      "Added data of 2014-06-23 00:00:00\n",
      "Added data of 2014-06-24 00:00:00\n",
      "Added data of 2014-06-25 00:00:00\n",
      "Added data of 2014-06-26 00:00:00\n",
      "Added data of 2014-06-27 00:00:00\n",
      "Added data of 2014-06-28 00:00:00\n",
      "Added data of 2014-06-29 00:00:00\n",
      "Added data of 2014-06-30 00:00:00\n",
      "Added data of 2014-07-01 00:00:00\n",
      "Added data of 2014-07-02 00:00:00\n",
      "Added data of 2014-07-03 00:00:00\n",
      "Added data of 2014-07-04 00:00:00\n",
      "Added data of 2014-07-05 00:00:00\n",
      "Added data of 2014-07-06 00:00:00\n",
      "Added data of 2014-07-07 00:00:00\n",
      "Added data of 2014-07-08 00:00:00\n",
      "Added data of 2014-07-09 00:00:00\n",
      "Added data of 2014-07-10 00:00:00\n",
      "Added data of 2014-07-11 00:00:00\n",
      "Added data of 2014-07-12 00:00:00\n",
      "Added data of 2014-07-13 00:00:00\n",
      "Added data of 2014-07-14 00:00:00\n",
      "Added data of 2014-07-15 00:00:00\n",
      "Added data of 2014-07-16 00:00:00\n",
      "Added data of 2014-07-17 00:00:00\n",
      "Added data of 2014-07-18 00:00:00\n",
      "Added data of 2014-07-19 00:00:00\n",
      "sleeping for 300 seconds...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "recent_anncs2df(20140101, 20200404, save=True, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
